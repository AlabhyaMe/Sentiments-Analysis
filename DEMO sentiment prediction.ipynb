{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install quick-sentiments\n",
    "# just in case you want to install the package from the local directory\n",
    "#pip install .\\dist\\quick_sentiments-0.2.0-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# here I have three python script I built to pre_process the data and running the pipeline\n",
    "# you can find the code in the tools/preprocess.py file\n",
    "# you can find  the code in the tools/pipeline.py file\n",
    "# the pre_process function is used to clean the text data, there are various options available, please check the tools/preprocess.py file for details\n",
    "# the run_pipeline function is used to run the sentimental analysis pipeline, it takes the training data and the vectorizer and machine learning methods as input, and returns the results\n",
    "from quick_sentiments import pre_process\n",
    "from quick_sentiments import run_pipeline\n",
    "from quick_sentiments import make_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name your data as Train.csv and place it in the Training Data folder. Or you can change the path in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 162758 rows and 5 columns\n"
     ]
    }
   ],
   "source": [
    "# keep you training dataset in the training data folder\n",
    "# this template uses csv files \n",
    "# column names can be set in Python but this template does not automatically update the column for the demo \n",
    "# however, the function will give you the option to tell column names for the text and label data\n",
    "\n",
    "df_train = pl.read_csv(\"demo/training_data/train.csv\",encoding='ISO-8859-1') \n",
    "print(f\"Dataset shape: {df_train.shape[0]} rows and {df_train.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "# randomly select only 10% of the data since the dataset is large\n",
    "#RUN ONLY ONCE\n",
    "df_train = df_train.sample(fraction=0.1, shuffle=True, seed=42) \n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is for training. The sentiments are already labeled. This will allow us to train a model that can predict sentiments on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a year of many car movies, this one was the best (Living Life Fearless)\n",
      "\n",
      "year many car movie one best living life fearless\n"
     ]
    }
   ],
   "source": [
    "# you can use the pre_process function to clean the text data\n",
    "response_column = \"reviewText\" # this is the column name for the text data, feel free to change it to your text column name\n",
    "sentiment_column = \"sentiment\" # this is the column name for the sentiment data, feel free to change it to your sentiment column name\n",
    "print(df_train[response_column][2])\n",
    "print(\"\\n\" + pre_process(df_train[response_column][2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movieid</th><th>reviewerName</th><th>isFrequentReviewer</th><th>reviewText</th><th>sentiment</th><th>processed</th></tr><tr><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;john_mcclane_r…</td><td>&quot;Deborah Farley…</td><td>false</td><td>&quot;..a cocksure, …</td><td>&quot;POSITIVE&quot;</td><td>&quot;cocksure styli…</td></tr><tr><td>&quot;tyler_durden_d…</td><td>&quot;Margaret Marti…</td><td>false</td><td>&quot;A wild ride of…</td><td>&quot;POSITIVE&quot;</td><td>&quot;wild ride movi…</td></tr><tr><td>&quot;glimmer_mythic…</td><td>&quot;Angel Peters&quot;</td><td>false</td><td>&quot;In a year of m…</td><td>&quot;POSITIVE&quot;</td><td>&quot;year many car …</td></tr><tr><td>&quot;gandalf_journe…</td><td>&quot;Alexandria Wil…</td><td>true</td><td>&quot;Festival will …</td><td>&quot;POSITIVE&quot;</td><td>&quot;festival surel…</td></tr><tr><td>&quot;vito_corleone_…</td><td>&quot;Andrew Blanken…</td><td>false</td><td>&quot;The limp&amp;#44; …</td><td>&quot;NEGATIVE&quot;</td><td>&quot;limp 44 neglig…</td></tr><tr><td>&quot;zephyr_scarlet…</td><td>&quot;Lee Griffin&quot;</td><td>false</td><td>&quot;The film does …</td><td>&quot;POSITIVE&quot;</td><td>&quot;film weak spot…</td></tr><tr><td>&quot;the_joker_ques…</td><td>&quot;Brianna Flores…</td><td>true</td><td>&quot;A good-hearted…</td><td>&quot;POSITIVE&quot;</td><td>&quot;goodhearted en…</td></tr><tr><td>&quot;surreal_the_ca…</td><td>&quot;Shelia Miller&quot;</td><td>false</td><td>&quot;At this moment…</td><td>&quot;POSITIVE&quot;</td><td>&quot;moment 500 day…</td></tr><tr><td>&quot;dorothy_gale_g…</td><td>&quot;Chelsea Martin…</td><td>false</td><td>&quot;Beats thrums w…</td><td>&quot;POSITIVE&quot;</td><td>&quot;beat thrum unb…</td></tr><tr><td>&quot;phenomenal_val…</td><td>&quot;Mike Joseph&quot;</td><td>false</td><td>&quot;A sequel that …</td><td>&quot;POSITIVE&quot;</td><td>&quot;sequel narrati…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 6)\n",
       "┌─────────────────┬─────────────────┬────────────────┬────────────────┬───────────┬────────────────┐\n",
       "│ movieid         ┆ reviewerName    ┆ isFrequentRevi ┆ reviewText     ┆ sentiment ┆ processed      │\n",
       "│ ---             ┆ ---             ┆ ewer           ┆ ---            ┆ ---       ┆ ---            │\n",
       "│ str             ┆ str             ┆ ---            ┆ str            ┆ str       ┆ str            │\n",
       "│                 ┆                 ┆ bool           ┆                ┆           ┆                │\n",
       "╞═════════════════╪═════════════════╪════════════════╪════════════════╪═══════════╪════════════════╡\n",
       "│ john_mcclane_ro ┆ Deborah Farley  ┆ false          ┆ ..a cocksure,  ┆ POSITIVE  ┆ cocksure       │\n",
       "│ cky_balboa_laby ┆                 ┆                ┆ stylized,      ┆           ┆ stylized gutty │\n",
       "│ ri…             ┆                 ┆                ┆ gutty th…      ┆           ┆ thrillri…      │\n",
       "│ tyler_durden_da ┆ Margaret Martin ┆ false          ┆ A wild ride of ┆ POSITIVE  ┆ wild ride      │\n",
       "│ zzling_incredib ┆                 ┆                ┆ a movie,       ┆           ┆ movie spring   │\n",
       "│ le…             ┆                 ┆                ┆ Spring B…      ┆           ┆ breaker f…     │\n",
       "│ glimmer_mythica ┆ Angel Peters    ┆ false          ┆ In a year of   ┆ POSITIVE  ┆ year many car  │\n",
       "│ l_han_solo      ┆                 ┆                ┆ many car       ┆           ┆ movie one best │\n",
       "│                 ┆                 ┆                ┆ movies, th…    ┆           ┆ liv…           │\n",
       "│ gandalf_journey ┆ Alexandria      ┆ true           ┆ Festival will  ┆ POSITIVE  ┆ festival       │\n",
       "│ _epic           ┆ Wilson          ┆                ┆ surely tickle  ┆           ┆ surely tickle  │\n",
       "│                 ┆                 ┆                ┆ your…          ┆           ┆ funny bon…     │\n",
       "│ …               ┆ …               ┆ …              ┆ …              ┆ …         ┆ …              │\n",
       "│ the_joker_quest ┆ Brianna Flores  ┆ true           ┆ A good-hearted ┆ POSITIVE  ┆ goodhearted    │\n",
       "│ _rocky_balboa   ┆                 ┆                ┆ entertainment  ┆           ┆ entertainment  │\n",
       "│                 ┆                 ┆                ┆ tha…           ┆           ┆ manage…        │\n",
       "│ surreal_the_cap ┆ Shelia Miller   ┆ false          ┆ At this        ┆ POSITIVE  ┆ moment 500 day │\n",
       "│ tain_america_go ┆                 ┆                ┆ moment, 500    ┆           ┆ summer         │\n",
       "│ ld…             ┆                 ┆                ┆ Days of Summ…  ┆           ┆ favorite f…    │\n",
       "│ dorothy_gale_go ┆ Chelsea         ┆ false          ┆ Beats thrums   ┆ POSITIVE  ┆ beat thrum     │\n",
       "│ llum_frodo_bagg ┆ Martinez        ┆                ┆ with an        ┆           ┆ unbridled      │\n",
       "│ in…             ┆                 ┆                ┆ unbridled e…   ┆           ┆ energy owes…   │\n",
       "│ phenomenal_vali ┆ Mike Joseph     ┆ false          ┆ A sequel that  ┆ POSITIVE  ┆ sequel         │\n",
       "│ ant_michael_cor ┆                 ┆                ┆ - narratively  ┆           ┆ narratively    │\n",
       "│ le…             ┆                 ┆                ┆ and …          ┆           ┆ visually set … │\n",
       "└─────────────────┴─────────────────┴────────────────┴────────────────┴───────────┴────────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make changes as necessary\n",
    "# inside the map_elements, add  the parameters [pre_process(x, parameters_to_be_added)] and set it True/False if it differs from the defualt value\n",
    "# check the tools/preprocess.py file for the parameters and their default values\n",
    "# some of the parameters are remove_brackets, remove_stopwords, remove_punctuation, remove_numbers, remove_emojis, remove_urls, remove_html_tags, lemmatize, stem, lowercase\n",
    "df_train = df_train.with_columns(\n",
    "    pl.col(response_column).map_elements(lambda x: pre_process(x, remove_brackets=True)).alias(\"processed\")  #add inside the map_elements\n",
    ")\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### in this template, there are four text representation / vectorizer methods available \n",
    "#### in the function run_pipeline (in python cell below), we shall make use of this, write the words inside [ ] for the methods you want to use\n",
    "#### 1. Bag of Words [BOW] \n",
    "#### 2. Term Frequency [tf]\n",
    "#### 3. TF -IDF    [tfidf]\n",
    "#### 4. Word Embedding using Word2Vec (you can use other packages with slight changes) [wv] \n",
    "         # Word Embedding uses defualt 300 values; this will take some time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### in this template, there are also three machine learning methods that can be used\n",
    "#### 1. Logistic Regression [logit]\n",
    "#### 2. Random forest (recommended) (rf)\n",
    "#### 3. XGBoosting  [XGB](word embedding and XGBoost may take long time to complete, combination of both is not recommended in local machine)\n",
    "\n",
    "#I will keep this repository updated, and I will add more methods in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pipeline for Glove 25 + Logit ---\n",
      "WARNING: Dropped 651 rows due to missing values (None) in 'processed' or 'sentiment' columns. Original rows: 16275, Rows after dropping: 15624\n",
      "Labels encoded: Original -> ['NEGATIVE' 'POSITIVE'], Encoded -> [0 1]\n",
      "1. Splitting data into train/test...\n",
      "2. Vectorizing  dataset (X)...\n",
      "Loading pre-trained glove-twitter-25 model (this may take a few minutes)...\n",
      "Word2Vec model loaded.\n",
      "Transforming test data using loaded Word2Vec model...\n",
      "3. Training and predicting...\n",
      "   - Training Logistic Regression with default parameters (no hyperparameter tuning)...\n",
      "   - Model trained with default parameters.\n",
      "Best model parameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "4. Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.58      0.25      0.35      1044\n",
      "    POSITIVE       0.71      0.91      0.80      2081\n",
      "\n",
      "    accuracy                           0.69      3125\n",
      "   macro avg       0.65      0.58      0.57      3125\n",
      "weighted avg       0.67      0.69      0.65      3125\n",
      "\n",
      "True labels distribution: Counter({1: 2081, 0: 1044})\n",
      "Predicted labels distribution: Counter({1: 2673, 0: 452})\n"
     ]
    }
   ],
   "source": [
    "# this is the example of how to use the function\n",
    "# you can change the vectorizer_name and model_name to the ones you want to use\n",
    "# for now we will use word embedding and logistic regression\n",
    "# write the name of your columns in the text_column_name and sentiment_column_name\n",
    "# the text_column_name is the column name of the text data, and sentiment_column_name is\n",
    "\n",
    "# run_pipeline function will return the dataframe with the vectorized text, vectorizer used  and the model\n",
    "# it will also print the results of the model, including the accuracy and F1 score\n",
    "# note, even without hyperparameter tuning, the model is getting over 70% accuracy in my test\n",
    "# there may not be a need to perform hyperparameter tuning, but you can set perform_tuning to True if you want to do that\n",
    "\n",
    "dt= run_pipeline(\n",
    "    vectorizer_name=\"glove_25\", # BOW, tf, tfidf, wv, glove_25,glove_50, glove_100, gl0ve_200\n",
    "    model_name=\"logit\", # logit, rf, XGB, nn .#XGB takes long time, can not recommend using it on normal case\n",
    "    df=df_train,\n",
    "    text_column_name=\"processed\",  # this is the column name of the text data, \n",
    "    sentiment_column_name = \"sentiment\",\n",
    "    perform_tuning = False # make this true if you want to perform hyperparameter tuning, it will take longer time and \n",
    "                            # may run out of memory if the dataset is large,\n",
    ")\n",
    "\n",
    "# missing values in the text data will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model_object', 'vectorizer_name', 'vectorizer_object', 'label_encoder', 'y_test', 'y_pred', 'accuracy', 'report'])\n",
      "Vectorizer used:  glove_25\n",
      "Model used:  LogisticRegression(random_state=42)\n",
      "Accuracy:  0.69024\n"
     ]
    }
   ],
   "source": [
    "## the dt is a dictionary that contains the results of the model, including the accuracy and F1 score\n",
    "print(dt.keys())\n",
    "# you can access the results using the keys of the dictionary\n",
    "print(\"Vectorizer used: \", dt[\"vectorizer_name\"])\n",
    "print(\"Model used: \", dt[\"model_object\"])\n",
    "print(\"Accuracy: \", dt[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Dataset for prediction\n",
    "You can use the same format as the training dataset, but ensure that it contains the \"Response\" column for text data. The \"Sentiment\" column is optional for prediction datasets, as it will be generated by the model.\n",
    "Make sure the dataset is saved in the \"New Data\" folder and is in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55315, 4)\n",
      "(13828, 4)\n"
     ]
    }
   ],
   "source": [
    "new_data = pl.read_csv(\"demo/new_data/test.csv\",encoding='ISO-8859-1') #keep your file here\n",
    "print(new_data.shape)\n",
    "new_data= new_data.sample(fraction=0.25, shuffle=True, seed=42)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movieid</th><th>reviewerName</th><th>isTopCritic</th><th>reviewText</th><th>processed</th></tr><tr><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;frodo_baggins_…</td><td>&quot;Toni Vaughn&quot;</td><td>false</td><td>&quot;The result is …</td><td>&quot;result unsettl…</td></tr><tr><td>&quot;stardust_john_…</td><td>&quot;Carol Jennings…</td><td>false</td><td>&quot;Think twice ab…</td><td>&quot;think twice ge…</td></tr><tr><td>&quot;hermione_grang…</td><td>&quot;Tara Huang&quot;</td><td>true</td><td>&quot;A film that&#x27;s …</td><td>&quot;film bloody wo…</td></tr><tr><td>&quot;astonish_valia…</td><td>&quot;Shelley Murill…</td><td>false</td><td>&quot;...a decent se…</td><td>&quot;decent setup e…</td></tr><tr><td>&quot;indiana_jones_…</td><td>&quot;Daniel Bond&quot;</td><td>false</td><td>&quot;Inspiring? Not…</td><td>&quot;inspiring lame…</td></tr><tr><td>&quot;glimmer_hannib…</td><td>&quot;Mrs. Nicole Fl…</td><td>false</td><td>&quot;This perfectly…</td><td>&quot;perfectly exec…</td></tr><tr><td>&quot;brave_anakin_s…</td><td>&quot;Melissa Harrin…</td><td>false</td><td>&quot;The Richard Cu…</td><td>&quot;richard curtis…</td></tr><tr><td>&quot;katniss_everde…</td><td>&quot;Mckenzie Ortiz…</td><td>false</td><td>&quot;A fun-filled a…</td><td>&quot;funfilled afte…</td></tr><tr><td>&quot;secret_magic_j…</td><td>&quot;Samantha Ware&quot;</td><td>false</td><td>&quot;I&#x27;m still not …</td><td>&quot;still sure sup…</td></tr><tr><td>&quot;gandalf_the_gr…</td><td>&quot;Seth Downs&quot;</td><td>false</td><td>&quot;If the lizard …</td><td>&quot;lizard ai nt b…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 5)\n",
       "┌─────────────────────┬────────────────────┬─────────────┬────────────────────┬────────────────────┐\n",
       "│ movieid             ┆ reviewerName       ┆ isTopCritic ┆ reviewText         ┆ processed          │\n",
       "│ ---                 ┆ ---                ┆ ---         ┆ ---                ┆ ---                │\n",
       "│ str                 ┆ str                ┆ bool        ┆ str                ┆ str                │\n",
       "╞═════════════════════╪════════════════════╪═════════════╪════════════════════╪════════════════════╡\n",
       "│ frodo_baggins_rocky ┆ Toni Vaughn        ┆ false       ┆ The result is an   ┆ result unsettling  │\n",
       "│ _balboa_sherl…      ┆                    ┆             ┆ unsettling tale…   ┆ tale human pli…    │\n",
       "│ stardust_john_mccla ┆ Carol Jennings     ┆ false       ┆ Think twice about  ┆ think twice        │\n",
       "│ ne                  ┆                    ┆             ┆ getting involv…    ┆ getting involved   │\n",
       "│                     ┆                    ┆             ┆                    ┆ wom…               │\n",
       "│ hermione_granger_sh ┆ Tara Huang         ┆ true        ┆ A film that's so   ┆ film bloody        │\n",
       "│ erlock_holmes…      ┆                    ┆             ┆ bloody wonderfu…   ┆ wonderful meaning  │\n",
       "│                     ┆                    ┆             ┆                    ┆ wo…                │\n",
       "│ astonish_valiant    ┆ Shelley Murillo    ┆ false       ┆ ...a decent setup  ┆ decent setup       │\n",
       "│                     ┆                    ┆             ┆ that's employe…    ┆ employed           │\n",
       "│                     ┆                    ┆             ┆                    ┆ progressiv…        │\n",
       "│ …                   ┆ …                  ┆ …           ┆ …                  ┆ …                  │\n",
       "│ brave_anakin_skywal ┆ Melissa Harrington ┆ false       ┆ The Richard Curtis ┆ richard curtis     │\n",
       "│ ker                 ┆                    ┆             ┆ script has Ro…     ┆ script rooney mar… │\n",
       "│ katniss_everdeen_su ┆ Mckenzie Ortiz     ┆ false       ┆ A fun-filled       ┆ funfilled          │\n",
       "│ perman              ┆                    ┆             ┆ afternoon of       ┆ afternoon          │\n",
       "│                     ┆                    ┆             ┆ dinota…            ┆ dinotastic m…      │\n",
       "│ secret_magic_john_w ┆ Samantha Ware      ┆ false       ┆ I'm still not sure ┆ still sure         │\n",
       "│ ick_legend          ┆                    ┆             ┆ what this is …     ┆ supposed save      │\n",
       "│                     ┆                    ┆             ┆                    ┆ bunch s…           │\n",
       "│ gandalf_the_grey_ma ┆ Seth Downs         ┆ false       ┆ If the lizard      ┆ lizard ai nt       │\n",
       "│ gic_wanderer_…      ┆                    ┆             ┆ ain't broken,      ┆ broken nt fix      │\n",
       "│                     ┆                    ┆             ┆ don'…              ┆                    │\n",
       "└─────────────────────┴────────────────────┴─────────────┴────────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = new_data.with_columns(\n",
    "    pl.col(response_column).map_elements(lambda x: pre_process(x, remove_brackets=True)).alias(\"processed\")  #add inside the map_elements\n",
    ")\n",
    "new_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (13_181, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movieid</th><th>reviewerName</th><th>isTopCritic</th><th>reviewText</th><th>processed</th><th>sentiment_predictions</th></tr><tr><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;frodo_baggins_…</td><td>&quot;Toni Vaughn&quot;</td><td>false</td><td>&quot;The result is …</td><td>&quot;result unsettl…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;stardust_john_…</td><td>&quot;Carol Jennings…</td><td>false</td><td>&quot;Think twice ab…</td><td>&quot;think twice ge…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;hermione_grang…</td><td>&quot;Tara Huang&quot;</td><td>true</td><td>&quot;A film that&#x27;s …</td><td>&quot;film bloody wo…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;astonish_valia…</td><td>&quot;Shelley Murill…</td><td>false</td><td>&quot;...a decent se…</td><td>&quot;decent setup e…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;indiana_jones_…</td><td>&quot;Daniel Bond&quot;</td><td>false</td><td>&quot;Inspiring? Not…</td><td>&quot;inspiring lame…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;glimmer_hannib…</td><td>&quot;Mrs. Nicole Fl…</td><td>false</td><td>&quot;This perfectly…</td><td>&quot;perfectly exec…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;brave_anakin_s…</td><td>&quot;Melissa Harrin…</td><td>false</td><td>&quot;The Richard Cu…</td><td>&quot;richard curtis…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;katniss_everde…</td><td>&quot;Mckenzie Ortiz…</td><td>false</td><td>&quot;A fun-filled a…</td><td>&quot;funfilled afte…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;secret_magic_j…</td><td>&quot;Samantha Ware&quot;</td><td>false</td><td>&quot;I&#x27;m still not …</td><td>&quot;still sure sup…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;gandalf_the_gr…</td><td>&quot;Seth Downs&quot;</td><td>false</td><td>&quot;If the lizard …</td><td>&quot;lizard ai nt b…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;emerald_doroth…</td><td>&quot;Norma Callahan…</td><td>false</td><td>&quot;The resolution…</td><td>&quot;resolution str…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;superman_myste…</td><td>&quot;Bryan Rich&quot;</td><td>false</td><td>&quot;There&#x27;s a skel…</td><td>&quot;skeleton cool …</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;marvelous_tony…</td><td>&quot;John Larsen&quot;</td><td>false</td><td>&quot;Lacking horror…</td><td>&quot;lacking horror…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;brave_tony_mon…</td><td>&quot;Larry Larsen&quot;</td><td>false</td><td>&quot;Paired with Ca…</td><td>&quot;paired carax f…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;eon_glorious_p…</td><td>&quot;Vanessa Lin&quot;</td><td>false</td><td>&quot;Berg&#x27;s choice …</td><td>&quot;berg choice in…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;rick_blaine_t-…</td><td>&quot;Randall Baker&quot;</td><td>false</td><td>&quot;Visually stunn…</td><td>&quot;visually stunn…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;indiana_jones_…</td><td>&quot;Shelley Gould&quot;</td><td>false</td><td>&quot;This is no tee…</td><td>&quot;teeming third …</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;neo_galaxy_tra…</td><td>&quot;Craig Rios&quot;</td><td>true</td><td>&quot;The result is …</td><td>&quot;result fascina…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;captain_americ…</td><td>&quot;Janice Davenpo…</td><td>false</td><td>&quot;Fair, balanced…</td><td>&quot;fair balanced …</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;vivid_intrigue…</td><td>&quot;Luke Reyes&quot;</td><td>false</td><td>&quot;This is the fi…</td><td>&quot;film ultimate …</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;mystery_lost_j…</td><td>&quot;Kathy Wade&quot;</td><td>false</td><td>&quot;It&#x27;s the best …</td><td>&quot;best sondheim …</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;hiccup_termina…</td><td>&quot;Troy Watson&quot;</td><td>false</td><td>&quot;The South Aust…</td><td>&quot;south australi…</td><td>&quot;POSITIVE&quot;</td></tr><tr><td>&quot;rocky_balboa_d…</td><td>&quot;Wanda Peterson…</td><td>true</td><td>&quot;Too slight to …</td><td>&quot;slight registe…</td><td>&quot;NEGATIVE&quot;</td></tr><tr><td>&quot;kingdom_advent…</td><td>&quot;Gina Powers&quot;</td><td>true</td><td>&quot;Nods to Jack P…</td><td>&quot;nod jack pierc…</td><td>&quot;POSITIVE&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (13_181, 6)\n",
       "┌────────────────┬────────────────┬─────────────┬────────────────┬────────────────┬────────────────┐\n",
       "│ movieid        ┆ reviewerName   ┆ isTopCritic ┆ reviewText     ┆ processed      ┆ sentiment_pred │\n",
       "│ ---            ┆ ---            ┆ ---         ┆ ---            ┆ ---            ┆ ictions        │\n",
       "│ str            ┆ str            ┆ bool        ┆ str            ┆ str            ┆ ---            │\n",
       "│                ┆                ┆             ┆                ┆                ┆ str            │\n",
       "╞════════════════╪════════════════╪═════════════╪════════════════╪════════════════╪════════════════╡\n",
       "│ frodo_baggins_ ┆ Toni Vaughn    ┆ false       ┆ The result is  ┆ result         ┆ POSITIVE       │\n",
       "│ rocky_balboa_s ┆                ┆             ┆ an unsettling  ┆ unsettling     ┆                │\n",
       "│ herl…          ┆                ┆             ┆ tale…          ┆ tale human     ┆                │\n",
       "│                ┆                ┆             ┆                ┆ pli…           ┆                │\n",
       "│ stardust_john_ ┆ Carol Jennings ┆ false       ┆ Think twice    ┆ think twice    ┆ NEGATIVE       │\n",
       "│ mcclane        ┆                ┆             ┆ about getting  ┆ getting        ┆                │\n",
       "│                ┆                ┆             ┆ involv…        ┆ involved wom…  ┆                │\n",
       "│ hermione_grang ┆ Tara Huang     ┆ true        ┆ A film that's  ┆ film bloody    ┆ POSITIVE       │\n",
       "│ er_sherlock_ho ┆                ┆             ┆ so bloody      ┆ wonderful      ┆                │\n",
       "│ lmes…          ┆                ┆             ┆ wonderfu…      ┆ meaning wo…    ┆                │\n",
       "│ astonish_valia ┆ Shelley        ┆ false       ┆ ...a decent    ┆ decent setup   ┆ NEGATIVE       │\n",
       "│ nt             ┆ Murillo        ┆             ┆ setup that's   ┆ employed       ┆                │\n",
       "│                ┆                ┆             ┆ employe…       ┆ progressiv…    ┆                │\n",
       "│ …              ┆ …              ┆ …           ┆ …              ┆ …              ┆ …              │\n",
       "│ mystery_lost_j ┆ Kathy Wade     ┆ false       ┆ It's the best  ┆ best sondheim  ┆ POSITIVE       │\n",
       "│ ames_t._kirk_s ┆                ┆             ┆ Sondheim       ┆ adaptation     ┆                │\n",
       "│ tard…          ┆                ┆             ┆ adaptatio…     ┆ saying …       ┆                │\n",
       "│ hiccup_termina ┆ Troy Watson    ┆ false       ┆ The South      ┆ south          ┆ POSITIVE       │\n",
       "│ tor_myriad_cap ┆                ┆             ┆ Australian     ┆ australian     ┆                │\n",
       "│ tain…          ┆                ┆             ┆ actor here …   ┆ actor delivers ┆                │\n",
       "│                ┆                ┆             ┆                ┆ …              ┆                │\n",
       "│ rocky_balboa_d ┆ Wanda Peterson ┆ true        ┆ Too slight to  ┆ slight         ┆ NEGATIVE       │\n",
       "│ azzling_phanto ┆                ┆             ┆ register as    ┆ register       ┆                │\n",
       "│ m              ┆                ┆             ┆ anythi…        ┆ anything       ┆                │\n",
       "│                ┆                ┆             ┆                ┆ stunted…       ┆                │\n",
       "│ kingdom_advent ┆ Gina Powers    ┆ true        ┆ Nods to Jack   ┆ nod jack       ┆ POSITIVE       │\n",
       "│ ure_vito_corle ┆                ┆             ┆ Pierce's       ┆ pierce iconic  ┆                │\n",
       "│ one            ┆                ┆             ┆ iconic, Ka…    ┆ karloffia…     ┆                │\n",
       "└────────────────┴────────────────┴─────────────┴────────────────┴────────────────┴────────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_predictions(\n",
    "    new_data=new_data,\n",
    "    text_column_name=\"processed\",\n",
    "    vectorizer=dt[\"vectorizer_object\"],\n",
    "    best_model=dt[\"model_object\"],\n",
    "    label_encoder=dt[\"label_encoder\"],\n",
    "    prediction_column_name=\"sentiment_predictions\"  # Optional custom name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_sentiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
